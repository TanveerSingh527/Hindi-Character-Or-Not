{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Import some modules\r\n",
    "import numpy as np\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Paths of all three folders\r\n",
    "train_dir = 'training' \r\n",
    "validation_dir = 'validation' # I randomly took 70-70 images of both classes from train folder and moved them into the validation folder\r\n",
    "test_dir = 'test' # It is not labelled, also Note that you need to create a seprate folder inside this folder and put all the images in that folder so that our generator can get images"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Here we are gettig our data from train and validation folders\r\n",
    "datagen = ImageDataGenerator(rescale=1/255) # We are just rescaling it\r\n",
    "\r\n",
    "# I used a target size of (150, 150) because it works well for me in my projects\r\n",
    "train_generator = datagen.flow_from_directory(\r\n",
    "        train_dir,\r\n",
    "        target_size=(150, 150),\r\n",
    "        batch_size=64,\r\n",
    "        class_mode='binary')\r\n",
    "\r\n",
    "validation_generator = datagen.flow_from_directory(\r\n",
    "        validation_dir,\r\n",
    "        target_size=(150, 150),\r\n",
    "        batch_size=5,\r\n",
    "        class_mode='binary')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 5735 images belonging to 2 classes.\n",
      "Found 140 images belonging to 2 classes.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from tensorflow.keras import models\r\n",
    "from tensorflow.keras import layers\r\n",
    "\r\n",
    "# We are using keras sequential api\r\n",
    "model = models.Sequential()\r\n",
    "\r\n",
    "\r\n",
    "# This is our model's architecture\r\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\r\n",
    "model.add(layers.MaxPooling2D((2, 2)))\r\n",
    "\r\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\n",
    "model.add(layers.MaxPooling2D((2, 2)))\r\n",
    "\r\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\r\n",
    "model.add(layers.MaxPooling2D((2, 2)))\r\n",
    "\r\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\r\n",
    "model.add(layers.MaxPooling2D((2, 2)))\r\n",
    "\r\n",
    "model.add(layers.Flatten())\r\n",
    "\r\n",
    "model.add(layers.Dense(256, activation='relu'))\r\n",
    "\r\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# You can see the summary using this line of code\r\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               1605888   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,846,977\n",
      "Trainable params: 1,846,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from tensorflow.keras import optimizers\r\n",
    "\r\n",
    "# Here we are compling our model, loss is binary crossentropy, optimizer is rmsprop with 1e-4 learning rate and metrics is accuracy\r\n",
    "model.compile(loss='binary_crossentropy', \r\n",
    "              optimizer=optimizers.RMSprop(1e-4),\r\n",
    "              metrics=['acc'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Here we are creating checkpoint of our model at every epoch. Note that thisis optional\r\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"Nvidia_Hackathon_Model_1-{epoch:02d}.h5\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Using this code cell you can start training your model, you can remove callbacks if you want\r\n",
    "# history = model.fit(\r\n",
    "#       train_generator,\r\n",
    "#       steps_per_epoch=64,\r\n",
    "#       epochs=50,\r\n",
    "#       callbacks=checkpoint_cb)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Here we are getting our test data\r\n",
    "test_generator = datagen.flow_from_directory(\r\n",
    "        test_dir,\r\n",
    "        target_size=(150, 150),\r\n",
    "        shuffle=False) # We set shuffle=False because we want that our predictions are in correct order"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 98 images belonging to 1 classes.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model = keras.models.load_model('model_name.h5') # Using this line you can load any of your alrready saved checkpoint\r\n",
    "# model.evaluate(validation_generator, steps=2) # You can use this line to evaluate your model on the validation data which we created manually"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "result= (model.predict(test_generator) > 0.5) # You can use this line of code to get your model's predictions on the test data\r\n",
    "# Note that our model will give prediction as False and True (False for no hindi text and True for hindi text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Here by using np.where we are replacing False and True with 0 and 1 respectively\r\n",
    "x = result\r\n",
    "result = np.where(x == False, 0, 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# This step is totally optional, if you want to export your predictions into a json file you need to convert them into a dictionary using this cell of code\r\n",
    "res = {}\r\n",
    "for i in range(1, 99):\r\n",
    "    test_set = str(i) + \".jpg\"\r\n",
    "    res[test_set] = int(result[i-1])\r\n",
    "# Note that for this step only we set shuffle=False, because we want our predictions to be in the correct order"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Using this cell you can export your predictions into a json file\r\n",
    "# import json\r\n",
    "# with open('predictions.json', 'w') as outfile:\r\n",
    "#     json.dump(res, outfile)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}